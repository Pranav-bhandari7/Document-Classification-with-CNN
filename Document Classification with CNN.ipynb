{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDMgSstPYv0P"
   },
   "source": [
    "# Text Classification:\n",
    "\n",
    "## Data\n",
    "<pre>\n",
    "1. we have total of 20 types of documents(Text files) and total 18828 documents(text files).\n",
    "2. You can download data from this <a href='https://drive.google.com/open?id=1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM'>link</a>, in that you will get documents.rar folder. <br>If you unzip that, you will get total of 18828 documnets. document name is defined as'ClassLabel_DocumentNumberInThatLabel'. \n",
    "so from document name, you can extract the label for that document.\n",
    "4. Now our problem is to classify all the documents into any one of the class.\n",
    "5. Below we provided count plot of all the labels in our data. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlqYFVI3Yv0k"
   },
   "source": [
    "#### sample document\n",
    "<pre>\n",
    "<font color='blue'>\n",
    "Subject: A word of advice\n",
    "From: jcopelan@nyx.cs.du.edu (The One and Only)\n",
    "\n",
    "In article < 65882@mimsy.umd.edu > mangoe@cs.umd.edu (Charley Wingate) writes:\n",
    ">\n",
    ">I've said 100 times that there is no \"alternative\" that should think you\n",
    ">might have caught on by now.  And there is no \"alternative\", but the point\n",
    ">is, \"rationality\" isn't an alternative either.  The problems of metaphysical\n",
    ">and religious knowledge are unsolvable-- or I should say, humans cannot\n",
    ">solve them.\n",
    "\n",
    "How does that saying go: Those who say it can't be done shouldn't interrupt\n",
    "those who are doing it.\n",
    "\n",
    "Jim\n",
    "--\n",
    "Have you washed your brain today?\n",
    "</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "\n",
    "%reload_ext tensorboard\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense,MaxPooling1D\n",
    "from keras.layers import Flatten,Dropout,concatenate\n",
    "from keras.layers import Embedding\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 18828/18828 [00:04<00:00, 4287.85it/s]\n"
     ]
    }
   ],
   "source": [
    "class_label = []\n",
    "content = []\n",
    "document_name = []\n",
    "\n",
    "for file in tqdm(os.listdir('documents/x/')):\n",
    "    document_name.append(file)\n",
    "    data = []\n",
    "    label = file.split('_')[0]\n",
    "    class_label.append(label)# here we are extracting class labels \n",
    "    \n",
    "    with open('documents/x/'+file,'r',encoding='utf8',errors='replace') as file:\n",
    "        data.append(file.read())\n",
    "    content.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18828"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {'document_name': document_name,\n",
    "     'doc_contents': content,\n",
    "     'class_label':class_label\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18828"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_name</th>\n",
       "      <th>doc_contents</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism_49960.txt</td>\n",
       "      <td>[From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: ...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism_51060.txt</td>\n",
       "      <td>[From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: ...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism_51119.txt</td>\n",
       "      <td>[From: I3150101@dbstu1.rz.tu-bs.de (Benedikt R...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism_51120.txt</td>\n",
       "      <td>[From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: ...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism_51121.txt</td>\n",
       "      <td>[From: strom@Watson.Ibm.Com (Rob Strom)\\nSubje...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           document_name                                       doc_contents  \\\n",
       "0  alt.atheism_49960.txt  [From: mathew <mathew@mantis.co.uk>\\nSubject: ...   \n",
       "1  alt.atheism_51060.txt  [From: mathew <mathew@mantis.co.uk>\\nSubject: ...   \n",
       "2  alt.atheism_51119.txt  [From: I3150101@dbstu1.rz.tu-bs.de (Benedikt R...   \n",
       "3  alt.atheism_51120.txt  [From: mathew <mathew@mantis.co.uk>\\nSubject: ...   \n",
       "4  alt.atheism_51121.txt  [From: strom@Watson.Ibm.Com (Rob Strom)\\nSubje...   \n",
       "\n",
       "   class_label  \n",
       "0  alt.atheism  \n",
       "1  alt.atheism  \n",
       "2  alt.atheism  \n",
       "3  alt.atheism  \n",
       "4  alt.atheism  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('preprocessed_data_file.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_name</th>\n",
       "      <th>doc_contents</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism_49960.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism_51060.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism_51119.txt</td>\n",
       "      <td>['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism_51120.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism_51121.txt</td>\n",
       "      <td>['From: strom@Watson.Ibm.Com (Rob Strom)\\nSubj...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           document_name                                       doc_contents  \\\n",
       "0  alt.atheism_49960.txt  ['From: mathew <mathew@mantis.co.uk>\\nSubject:...   \n",
       "1  alt.atheism_51060.txt  ['From: mathew <mathew@mantis.co.uk>\\nSubject:...   \n",
       "2  alt.atheism_51119.txt  ['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...   \n",
       "3  alt.atheism_51120.txt  ['From: mathew <mathew@mantis.co.uk>\\nSubject:...   \n",
       "4  alt.atheism_51121.txt  ['From: strom@Watson.Ibm.Com (Rob Strom)\\nSubj...   \n",
       "\n",
       "   class_label  \n",
       "0  alt.atheism  \n",
       "1  alt.atheism  \n",
       "2  alt.atheism  \n",
       "3  alt.atheism  \n",
       "4  alt.atheism  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pd.read_csv('preprocessed_data_file.csv')\n",
    "preprocessed_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "sns.countplot(y = preprocessed_data['class_label'])\n",
    "plt.show()   # here we got fairly balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:\n",
    "<pre>\n",
    "useful links: <a href='http://www.pyregex.com/'>http://www.pyregex.com/</a>\n",
    "\n",
    "<font color='blue'><b>1.</b></font> Find all emails in the document and then get the text after the \"@\". and then split those texts by '.' \n",
    "after that remove the words whose length is less than or equal to 2 and also remove'com' word and then combine those words by space. \n",
    "In one doc, if we have 2 or more mails, get all.\n",
    "<b>Eg:[test@dm1.d.com, test2@dm2.dm3.com]-->[dm1.d.com, dm3.dm4.com]-->[dm1,d,com,dm2,dm3,com]-->[dm1,dm2,dm3]-->\"dm1 dm2 dm3\" </b> \n",
    "append all those into one list/array. ( This will give length of 18828 sentences i.e one list for each of the document). \n",
    "Some sample output was shown below. \n",
    "\n",
    "> In the above sample document there are emails [jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu]\n",
    "\n",
    "preprocessing:\n",
    "[jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu] ==> [nyx cs du edu mimsy umd edu cs umd edu] ==> \n",
    "[nyx edu mimsy umd edu umd edu]\n",
    "\n",
    "<font color='blue'><b>2.</b></font> Replace all the emails by space in the original text. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfWUeIN1Yv1N"
   },
   "source": [
    "### To get above mentioned data frame --> Try to Write Total Preprocessing steps in One Function Named Preprocess as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEGEHTNQYv1N"
   },
   "outputs": [],
   "source": [
    "def preprocess(input_content):\n",
    "    \"\"\"Do all the Preprocessing as shown above and\n",
    "    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data\"\"\"\n",
    "    \n",
    "    email = []  # store contents of the email in list\n",
    "    list_of_preproessed_emails = \"\"  # store contents of the email in string\n",
    "    \n",
    "    for em in re.findall(r'[\\w\\-\\.]+@[\\w\\.-]+\\b', input_content):  #extracting email \n",
    "        input_content = re.sub(em,'',input_content)\n",
    "        \n",
    "        email_temp =[]\n",
    "        email_temp = em.split('@')[1]      #taking text after @\n",
    "        email_temp = email_temp.split('.')      # a list containing words split by \".\"  \n",
    "\n",
    "        if 'com' in email_temp:\n",
    "            email_temp.remove('com')\n",
    " \n",
    "        for k in email_temp:            # removing words less than 3\n",
    "              if len(k)>2:\n",
    "                  email.append(k)\n",
    "    \n",
    "    for i in email:            #joining all the words in a string\n",
    "        list_of_preproessed_emails += i\n",
    "        list_of_preproessed_emails += ' '\n",
    "        \n",
    "    subject = re.search(r'Subject:(.*?)\\\\n', input_content).group(1) \n",
    "    subject =  re.sub(r'\\\\t','',subject)  #finding the main text of sbject \n",
    "    input_content = re.sub(r'Subject:','',input_content)  # removing the Subject keyword\n",
    "\n",
    "    input_content = re.sub(r'<(.*?)>','',input_content)    # removing tags like <,>\n",
    "    input_content = re.sub(r'<','',input_content)\n",
    "    input_content = re.sub(r'>','',input_content)\n",
    "    \n",
    "    input_content = re.sub(r'\\((.*?)\\)','',input_content)  # removing tags like (,)\n",
    "    input_content = re.sub(r'\\(','',input_content)\n",
    "    input_content = re.sub(r'\\)','',input_content)\n",
    "    \n",
    "    input_content = re.sub(r'Write to:(.*?)\\\\n','',input_content) # removing text after Write to: in a particular sentence \n",
    "    input_content = re.sub(r'Write to:','',input_content) # removing the Write to: keyword \n",
    "     \n",
    "    input_content = re.sub(r'From:(.*?)\\\\n','',input_content) # removing text after From: in a particular sentence \n",
    "    input_content = re.sub(r'From:','',input_content)  # removing the From: keyword \n",
    "    \n",
    "    input_content = input_content.replace('\\\\n','') # removing all the newlines('\\n')\n",
    "    input_content = input_content.replace('\\\\t','') # removing all the tabs('\\t')\n",
    "    input_content = input_content.replace('-','') # removing all the \"-\"\n",
    "    input_content = input_content.replace('\\\\','') # removing all the \"\\\"\n",
    "    input_content = input_content.replace('/',' ') # removing all the \"/\"\n",
    "    \n",
    "    input_content = re.sub(r'\\w+:\\s?','',input_content)  # removing all the words which ends with \":\"\n",
    "    \n",
    "    input_content = input_content.replace(\"won't\", 'will not')  # Decontractions, replaceing words to their fullforms.\n",
    "    input_content = input_content.replace(\"don't\", 'do not')\n",
    "    input_content = input_content.replace(\"'s\", ' is')\n",
    "    input_content = input_content.replace(\"'m\",' am')\n",
    "    input_content = input_content.replace(\"'re\", ' are')\n",
    "    input_content = input_content.replace(\"'ve\", ' have')\n",
    "    input_content = input_content.replace(\"n't\", ' not')\n",
    "    input_content = input_content.replace(\"'ll\", ' will') \n",
    "    input_content = input_content.replace(\"should n't\", 'should not')\n",
    "    input_content = input_content.replace(\"can't\", 'can not') \n",
    "    \n",
    "    \n",
    "    tree = nltk.ne_chunk(nltk.pos_tag(input_content.split()))  # TEXT chunking on the text\n",
    "    \n",
    "    GPE_arr=[]\n",
    "    for tree in tree.subtrees(filter=lambda z: z.label() == 'GPE' ):\n",
    "        for features in tree.leaves():\n",
    "            (exp, label) = features\n",
    "            if label == 'NNP':\n",
    "                GPE_arr.append(exp)\n",
    "                \n",
    "    PERSON_arr=[]            \n",
    "    for tree in tree.subtrees(filter=lambda z: z.label() == 'PERSON' ):\n",
    "        for features in tree.leaves():\n",
    "            (exp, label) = features\n",
    "            if label =='NNP':\n",
    "                PERSON_arr.append(exp)\n",
    "                \n",
    "                \n",
    "    for person in PERSON_arr:\n",
    "        input_content = input_content.replace(person,\"\")\n",
    "        \n",
    "    temp = '_'.join(GPE_arr)\n",
    "    input_content = input_content.replace(temp,\"\")\n",
    "    \n",
    "    \n",
    "    input_content = re.sub(r'[0-9]','',input_content)  # removing all the digits\n",
    "    \n",
    "    input_content = re.sub(re.compile(r'_+[a-zA-Z+]+_'),'',input_content) # removing all the words like _word_\n",
    "    input_content = re.sub(re.compile(r'_+[a-zA-Z+]'),'',input_content) # removing all the words like _word\n",
    "    input_content = re.sub(re.compile(r'[a-zA-Z+]+_'),'',input_content) # removing all the words like word_\n",
    "    \n",
    "    matches = re.findall(r'[a-zA-Z]+_+[a-zA-Z]+',input_content)   # words with underscore\n",
    "    \n",
    "    for match in matches:       # removing the words which are length less than or equal to 2\n",
    "        arr = match.split('_')\n",
    "        for i in arr:\n",
    "            if len(i)<=2:\n",
    "                arr.remove(i)\n",
    "                arr = ' '.join(arr)\n",
    "                input_content = input_content.replace(str(match),str(arr))\n",
    "     \n",
    "    input_content = input_content.lower()  # converting to lower case\n",
    "    input_content=input_content.split(' ')\n",
    "    \n",
    "    for word in input_content:     # removing the words >= 15 or <= 2\n",
    "        if (len(word)<3 or len(word)>15):\n",
    "            input_content.remove(word)\n",
    "    input_content = \" \".join(input_content)\n",
    "    input_content  = re.sub(re.compile(r'[^a-zA-Z_]'),' ',input_content)\n",
    "   \n",
    "    return (list_of_preproessed_emails,subject,input_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email:- mantis netcom mantis  \n",
      "Subject:-  Alt.Atheism FAQ: Atheist Resources \n",
      "Text:- alt atheism atheist atheism december atheist resources addresses atheist organizations usafreedom from religion fish bumper stickers and assorted other atheist paraphernalia areavailable from the freedom from religion foundation the us  evolution designs sell the  darwin fish   fish symbol  like the oneschristians stick their cars  but with feet and the word  darwin  writteninside  the deluxe moulded plastic fish    postpaid the us   people the san francisco bay area can get darwin fish from lynn gold try mailing for net people who lynn directly  theprice    per fish american atheist pressaap publish various atheist books critiques the bible  lists ofbiblical contradictions  and on  one such book  the bible handbook  w p  ball and g w  foote  american atheist press  pp  isbn nd edition  bible atrocities  immoralities    contains ball   the itself   aap  based the king james version the bible  cameron road  austin  prometheus bookssell books including haught  holy horrors   an alternate address prometheus books  glenn drive  buffalo   africanamericans for humanisman organization promoting black secular humanism and uncovering the history ofblack freethought  they publish quarterly newsletter  aah examiner  buffalo  united press association national secular society islington high street holloway roadlondon ew london nl british humanist association south place ethical society lamb conduit passage conway halllondon wcr red lion square                            london wcr rlfax                            the national secular society publish  the freethinker   monthly magazinefounded                                   germanyibka bund der und berlin germany ibka publish miz  mizvertrieb  postfach berlin germany for atheist books  write ibdk  internationaler b ucherdienst der   d hannover   germany                                 books  fictionthomas disch the santa claus story   the ultimate proof that santa exists   all characters and events are fictitious   any similarity living dead gods  uh  well   walter miller  jr a canticle for leibowitz one gem this post atomic doomsday novel the monks who spent their livescopying blueprints from  saint leibowitz   filling the sheets paper withink and leaving white lines and letters edgar atomic doomsday novel set clerical states   the church  for example forbids that anyone  produce  describe use any substance philip dickphilip dick dick wrote many philosophical and short stories and novels   his stories are bizarre times  but very approachable he wrote mainly sf  but wrote about people  truth and religion rather thantechnology   although often believed that had met some sort god  heremained sceptical   amongst his novels  the following are some  galactic pothealer a fallible alien deity summons group earth craftsmen and women aremote planet raise giant cathedral from beneath the oceans   when thedeity begins demand faith from the earthers  pothealer joe fernwright isunable comply   polished  ironic and amusing novel  a maze death noteworthy for its description technologybased schizophrenic hero searches for the hidden mysteries gnosticchristianity after reality fired into his brain pink laser beam ofunknown but possibly divine origin   accompanied his dogmatic anddismissively atheist friend and assorted other odd characters  the divine invasion god invades earth making young woman pregnant she returns fromanother star system   unfortunately she terminally ill  and must beassisted dead man whose brain wired hour easy listening music margaret atwood the handmaid tale a story based the premise that the congress mysteriouslyassassinated  and fundamentalists quickly take charge the nation set it right  again   the book the diary woman life she tries liveunder the new christian theocracy   women right own property revoked and their bank accounts are closed  sinful luxuries are outlawed  and theradio only used for readings from the bible   crimes are doctors who performed legal abortions the  old world  arehunted down and hanged   atwood writing style difficult get used toat first  but the tale grows more and more chilling it goes on various authors the bible this somewhat dull and rambling work has often been criticized   however  itis probably worth reading  only that you will know what all the fuss isabout   it exists many different versions  make sure you get the onetrue version                              books  nonfictionpeter rosa vicars christ   bantam press  although rosa seems christian even catholic this veryenlighting history papal immoralities  adulteries  fallacies etc michael martin a philosophical justification   temple university press  philadelphia  usa a detailed and scholarly justification atheism   contains outstandingappendix defining terminology and usage this  tendentiousarea   argues both for  negative atheism  and also for  positive atheism   includes great refutations the mostchallenging arguments for god  particular attention paid refutingcontempory theists such platinga and swinburne  pages  isbn   the case against christianity   temple university pressa comprehensive critique christianity  which he considersthe best contemporary defences christianity and demonstrates that they are unsupportable and incoherent  pages  isbn james turner without god  without creed   the johns hopkins university press  baltimore  md  usasubtitled  the origins unbelief america    examines the way whichunbelief   became mainstream  focusses the period   and while considering franceand britain the emphasis american  and particularly new   neither religious history secularization atheism without god  without creed is  rather  the intellectual history the fateof a single idea  the belief that god exists    pages  isbn  x  george seldes  the great thoughts   ballantine books  new york  usaa  dictionary quotations  a different kind  concentrating statementsand writings which  explicitly implicitly  present the person philosophyand worldview   includes obscure  opinions from manypeople   for some popular observations  traces the way which variouspeople expressed and twisted the idea over the centuries   quite a number ofthe quotations are derived from cardiff  what great men think religion and noyes   views religion   pages  isbn  x richard swinburne the existence god clarendon paperbacks  oxfordthis book the second volume a trilogy that began with  the coherence oftheism   and was concluded with  faith and reason     thiswork  swinburne attempts construct a series inductive arguments for theexistence god   his arguments  which are somewhat tendentious and relyupon the imputation late century western christian values andaesthetics a god which supposedly simple can be conceived  weredecisively rejected mackie  the miracle theism    the revisededition  the existence god   swinburne includes appendix in which hemakes a somewhat incoherent attempt rebut mackie j  mackie the miracle theism   oxfordthis  volume contains a comprehensive review the for and against the existence god   it ranges from the positions descartes  anselm  berkeley  hume al  throughthe moral arguments newman  kant and sidgwick  the recent restatementsof the classical theses plantinga and swinburne   it also addresses thosepositions which push the concept god beyond the realm the rational such those kierkegaard  kung and philips  well  replacements forgod  such lelie axiarchism   the book a delight read  lessformalistic and better written than martin works  and refreshingly directwhen compared with the handwaving swinburne james haught holy illustrated history religious murder and madness   prometheus bookslooks religious persecution from ancient times the present day  andnot only christians library congress catalog card number    norm allen  jr  african american anthology see the listing for african americans for humanism above gordon stein an anthology atheism and rationalism   prometheus booksan anthology covering a wide range subjects  including  the devil  eviland morality  and  the history freethought    comprehensive d  cohen the mind the biblebeliever   prometheus booksa study why people become christian and what effect ithas on them                                 net resourcesthere a small mailbased archive server mantis co uk which carriesarchives old articles and assorted other files   formore information  send mail to  saying   help   send atheism indexand it will mail back a reply mathew   \n"
     ]
    }
   ],
   "source": [
    "email_example,subject_example,Text_example = preprocess(np.array(preprocessed_data[preprocessed_data['document_name']==\"alt.atheism_49960.txt\"]['doc_contents'])[0])\n",
    "print('Email:-',email_example,'\\nSubject:-',subject_example,'\\nText:-',Text_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2x3og_iaYv1S"
   },
   "source": [
    "### After writing Preprocess function, call the function for each of the document(18828 docs) and then create a dataframe as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 18828/18828 [13:03<00:00, 24.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_name</th>\n",
       "      <th>doc_contents</th>\n",
       "      <th>class_label</th>\n",
       "      <th>preprocessed_subject</th>\n",
       "      <th>preprocessed_email</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism_49960.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Alt.Atheism FAQ: Atheist Resources</td>\n",
       "      <td>mantis netcom mantis</td>\n",
       "      <td>alt atheism atheist atheism december atheist r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism_51060.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Alt.Atheism FAQ: Introduction to Atheism</td>\n",
       "      <td>mantis mantis mantis</td>\n",
       "      <td>alt atheism introduction atheism april  begin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism_51119.txt</td>\n",
       "      <td>['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Re: Gospel Dating</td>\n",
       "      <td>dbstu1 tu-bs mimsy umd edu umd edu</td>\n",
       "      <td>gospel datingin article well  john has quite d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism_51120.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Re: university violating separation of church...</td>\n",
       "      <td>mantis kepler unh edu</td>\n",
       "      <td>university violating separation church state  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism_51121.txt</td>\n",
       "      <td>['From: strom@Watson.Ibm.Com (Rob Strom)\\nSubj...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Re: [soc.motss, et al.] \"Princeton axes match...</td>\n",
       "      <td>Watson Ibm Com harder ccr-p ida org harder ccr...</td>\n",
       "      <td>soc motss  al    princeton axes matching fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18823</th>\n",
       "      <td>talk.religion.misc_84564.txt</td>\n",
       "      <td>['From: sbuckley@fraser.sfu.ca (Stephen Buckle...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>Re: Religion and marriage</td>\n",
       "      <td>fraser sfu magnus acs ohio-state edu</td>\n",
       "      <td>religion and marriage was not sure this was th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18824</th>\n",
       "      <td>talk.religion.misc_84565.txt</td>\n",
       "      <td>['From: bakerj@gtephx.UUCP (Jon Baker)\\nSubjec...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>Re: A Message for you Mr. President: How do y...</td>\n",
       "      <td>gtephx UUCP ifi uio ifi uio ncratl AtlantaGA N...</td>\n",
       "      <td>message for you you know what happened in arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18825</th>\n",
       "      <td>talk.religion.misc_84568.txt</td>\n",
       "      <td>[\"From: pharvey@quack.kfu.com (Paul Harvey)\\nS...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>Re: Why did they behave as they did (Waco--re...</td>\n",
       "      <td>quack kfu emx utexas edu emx utexas edu</td>\n",
       "      <td>why did they behave they did article  if you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18826</th>\n",
       "      <td>talk.religion.misc_84569.txt</td>\n",
       "      <td>['From: &lt;KEVXU@CUNYVM.BITNET&gt;\\nSubject: Re: In...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>Re: Info about New Age!</td>\n",
       "      <td>CUNYVM BITNET digi lonestar org digi lonestar ...</td>\n",
       "      <td>info about new age in article the danger antic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18827</th>\n",
       "      <td>talk.religion.misc_84570.txt</td>\n",
       "      <td>[\"From: pharvey@quack.kfu.com (Paul Harvey)\\nS...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>Re: I'll see your demand and raise you... (wa...</td>\n",
       "      <td>quack kfu darkside osrhe uoknor edu okcforum o...</td>\n",
       "      <td>will see your demand and raise you    article ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18828 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      document_name  \\\n",
       "0             alt.atheism_49960.txt   \n",
       "1             alt.atheism_51060.txt   \n",
       "2             alt.atheism_51119.txt   \n",
       "3             alt.atheism_51120.txt   \n",
       "4             alt.atheism_51121.txt   \n",
       "...                             ...   \n",
       "18823  talk.religion.misc_84564.txt   \n",
       "18824  talk.religion.misc_84565.txt   \n",
       "18825  talk.religion.misc_84568.txt   \n",
       "18826  talk.religion.misc_84569.txt   \n",
       "18827  talk.religion.misc_84570.txt   \n",
       "\n",
       "                                            doc_contents         class_label  \\\n",
       "0      ['From: mathew <mathew@mantis.co.uk>\\nSubject:...         alt.atheism   \n",
       "1      ['From: mathew <mathew@mantis.co.uk>\\nSubject:...         alt.atheism   \n",
       "2      ['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...         alt.atheism   \n",
       "3      ['From: mathew <mathew@mantis.co.uk>\\nSubject:...         alt.atheism   \n",
       "4      ['From: strom@Watson.Ibm.Com (Rob Strom)\\nSubj...         alt.atheism   \n",
       "...                                                  ...                 ...   \n",
       "18823  ['From: sbuckley@fraser.sfu.ca (Stephen Buckle...  talk.religion.misc   \n",
       "18824  ['From: bakerj@gtephx.UUCP (Jon Baker)\\nSubjec...  talk.religion.misc   \n",
       "18825  [\"From: pharvey@quack.kfu.com (Paul Harvey)\\nS...  talk.religion.misc   \n",
       "18826  ['From: <KEVXU@CUNYVM.BITNET>\\nSubject: Re: In...  talk.religion.misc   \n",
       "18827  [\"From: pharvey@quack.kfu.com (Paul Harvey)\\nS...  talk.religion.misc   \n",
       "\n",
       "                                    preprocessed_subject  \\\n",
       "0                     Alt.Atheism FAQ: Atheist Resources   \n",
       "1               Alt.Atheism FAQ: Introduction to Atheism   \n",
       "2                                      Re: Gospel Dating   \n",
       "3       Re: university violating separation of church...   \n",
       "4       Re: [soc.motss, et al.] \"Princeton axes match...   \n",
       "...                                                  ...   \n",
       "18823                          Re: Religion and marriage   \n",
       "18824   Re: A Message for you Mr. President: How do y...   \n",
       "18825   Re: Why did they behave as they did (Waco--re...   \n",
       "18826                            Re: Info about New Age!   \n",
       "18827   Re: I'll see your demand and raise you... (wa...   \n",
       "\n",
       "                                      preprocessed_email  \\\n",
       "0                                  mantis netcom mantis    \n",
       "1                                  mantis mantis mantis    \n",
       "2                    dbstu1 tu-bs mimsy umd edu umd edu    \n",
       "3                                 mantis kepler unh edu    \n",
       "4      Watson Ibm Com harder ccr-p ida org harder ccr...   \n",
       "...                                                  ...   \n",
       "18823              fraser sfu magnus acs ohio-state edu    \n",
       "18824  gtephx UUCP ifi uio ifi uio ncratl AtlantaGA N...   \n",
       "18825           quack kfu emx utexas edu emx utexas edu    \n",
       "18826  CUNYVM BITNET digi lonestar org digi lonestar ...   \n",
       "18827  quack kfu darkside osrhe uoknor edu okcforum o...   \n",
       "\n",
       "                                       preprocessed_text  \n",
       "0      alt atheism atheist atheism december atheist r...  \n",
       "1      alt atheism introduction atheism april  begin ...  \n",
       "2      gospel datingin article well  john has quite d...  \n",
       "3      university violating separation church state  ...  \n",
       "4       soc motss  al    princeton axes matching fund...  \n",
       "...                                                  ...  \n",
       "18823  religion and marriage was not sure this was th...  \n",
       "18824  message for you you know what happened in arti...  \n",
       "18825  why did they behave they did article  if you w...  \n",
       "18826  info about new age in article the danger antic...  \n",
       "18827  will see your demand and raise you    article ...  \n",
       "\n",
       "[18828 rows x 6 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text,preprocessed_subject,preprocessed_emails = [],[],[]\n",
    "\n",
    "for i in tqdm(np.array(preprocessed_data['doc_contents'])):\n",
    "    email,subject,text = preprocess(i)\n",
    "    preprocessed_emails.append(email)\n",
    "    preprocessed_subject.append(subject)\n",
    "    preprocessed_text.append(text)\n",
    "\n",
    "preprocessed_data['preprocessed_subject'] = preprocessed_subject\n",
    "preprocessed_data['preprocessed_email'] = preprocessed_emails\n",
    "preprocessed_data['preprocessed_text'] = preprocessed_text\n",
    "\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3ucJLtWYv1V"
   },
   "source": [
    "### Training The models to Classify: \n",
    "\n",
    "<pre>\n",
    "1. Combine \"preprocessed_text\", \"preprocessed_subject\", \"preprocessed_emails\" into one column. use that column to model. \n",
    "\n",
    "2. Now Split the data into Train and test. use 25% for test also do a stratify split. \n",
    "\n",
    "3. Analyze your text data and pad the sequnce if required. \n",
    "Sequnce length is not restricted, you can use anything of your choice. \n",
    "you need to give the reasoning\n",
    "\n",
    "4. Do Tokenizer i.e convert text into numbers. please be careful while doing it. \n",
    "if you are using tf.keras \"Tokenizer\" API, it removes the <b>\"_\"</b>, but we need that.\n",
    "\n",
    "5. code the model's ( Model-1, Model-2 ) as discussed below \n",
    "and try to optimize that models.  \n",
    "\n",
    "6. For every model use predefined Glove vectors. \n",
    "<b>Don't train any word vectors while Training the model.</b>\n",
    "\n",
    "7. Use \"categorical_crossentropy\" as Loss. \n",
    "\n",
    "8. Use <b>Accuracy and Micro Avgeraged F1 score</b> as your as Key metrics to evaluate your model. \n",
    "\n",
    "9.  Use Tensorboard to plot the loss and Metrics based on the epoches.\n",
    "\n",
    "10. Please save your best model weights in to <b>'best_model_L.h5' ( L = 1 or 2 )</b>. \n",
    "\n",
    "11. You are free to choose any Activation function, learning rate, optimizer.\n",
    "But have to use the same architecture which we are giving below.\n",
    "\n",
    "12. You can add some layer to our architecture but you <b>deletion</b> of layer is not acceptable.\n",
    "\n",
    "13. Try to use <b>Early Stopping</b> technique or any of the callback techniques that you did in the previous assignments.\n",
    "\n",
    "14. For Every model save your model to image ( Plot the model) with shapes \n",
    "and inlcude those images in the notebook markdown cell, \n",
    "upload those imgages to Classroom. You can use \"plot_model\" \n",
    "please refer <a href='https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model'>this</a> if you don't know how to plot the model with shapes. \n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Combine \"preprocessed_text\", \"preprocessed_subject\", \"preprocessed_emails\" into one column. use that column to model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data['preprocessed_data'] = preprocessed_data['preprocessed_subject']+' ' + preprocessed_data['preprocessed_email']+ ' ' +preprocessed_data['preprocessed_text']\n",
    "preprocessed_data.to_csv('preprocessed_data_file2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_name</th>\n",
       "      <th>doc_contents</th>\n",
       "      <th>class_label</th>\n",
       "      <th>preprocessed_subject</th>\n",
       "      <th>preprocessed_email</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>preprocessed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism_49960.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Alt.Atheism FAQ: Atheist Resources</td>\n",
       "      <td>mantis netcom mantis</td>\n",
       "      <td>alt atheism atheist atheism december atheist r...</td>\n",
       "      <td>Alt.Atheism FAQ: Atheist Resources mantis net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism_51060.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Alt.Atheism FAQ: Introduction to Atheism</td>\n",
       "      <td>mantis mantis mantis</td>\n",
       "      <td>alt atheism introduction atheism april  begin ...</td>\n",
       "      <td>Alt.Atheism FAQ: Introduction to Atheism mant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism_51119.txt</td>\n",
       "      <td>['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Re: Gospel Dating</td>\n",
       "      <td>dbstu1 tu-bs mimsy umd edu umd edu</td>\n",
       "      <td>gospel datingin article well  john has quite d...</td>\n",
       "      <td>Re: Gospel Dating dbstu1 tu-bs mimsy umd edu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism_51120.txt</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Re: university violating separation of church...</td>\n",
       "      <td>mantis kepler unh edu</td>\n",
       "      <td>university violating separation church state  ...</td>\n",
       "      <td>Re: university violating separation of church...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism_51121.txt</td>\n",
       "      <td>['From: strom@Watson.Ibm.Com (Rob Strom)\\nSubj...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Re: [soc.motss, et al.] \"Princeton axes match...</td>\n",
       "      <td>Watson Ibm Com harder ccr-p ida org harder ccr...</td>\n",
       "      <td>soc motss  al    princeton axes matching fund...</td>\n",
       "      <td>Re: [soc.motss, et al.] \"Princeton axes match...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           document_name                                       doc_contents  \\\n",
       "0  alt.atheism_49960.txt  ['From: mathew <mathew@mantis.co.uk>\\nSubject:...   \n",
       "1  alt.atheism_51060.txt  ['From: mathew <mathew@mantis.co.uk>\\nSubject:...   \n",
       "2  alt.atheism_51119.txt  ['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...   \n",
       "3  alt.atheism_51120.txt  ['From: mathew <mathew@mantis.co.uk>\\nSubject:...   \n",
       "4  alt.atheism_51121.txt  ['From: strom@Watson.Ibm.Com (Rob Strom)\\nSubj...   \n",
       "\n",
       "   class_label                               preprocessed_subject  \\\n",
       "0  alt.atheism                 Alt.Atheism FAQ: Atheist Resources   \n",
       "1  alt.atheism           Alt.Atheism FAQ: Introduction to Atheism   \n",
       "2  alt.atheism                                  Re: Gospel Dating   \n",
       "3  alt.atheism   Re: university violating separation of church...   \n",
       "4  alt.atheism   Re: [soc.motss, et al.] \"Princeton axes match...   \n",
       "\n",
       "                                  preprocessed_email  \\\n",
       "0                              mantis netcom mantis    \n",
       "1                              mantis mantis mantis    \n",
       "2                dbstu1 tu-bs mimsy umd edu umd edu    \n",
       "3                             mantis kepler unh edu    \n",
       "4  Watson Ibm Com harder ccr-p ida org harder ccr...   \n",
       "\n",
       "                                   preprocessed_text  \\\n",
       "0  alt atheism atheist atheism december atheist r...   \n",
       "1  alt atheism introduction atheism april  begin ...   \n",
       "2  gospel datingin article well  john has quite d...   \n",
       "3  university violating separation church state  ...   \n",
       "4   soc motss  al    princeton axes matching fund...   \n",
       "\n",
       "                                   preprocessed_data  \n",
       "0   Alt.Atheism FAQ: Atheist Resources mantis net...  \n",
       "1   Alt.Atheism FAQ: Introduction to Atheism mant...  \n",
       "2   Re: Gospel Dating dbstu1 tu-bs mimsy umd edu ...  \n",
       "3   Re: university violating separation of church...  \n",
       "4   Re: [soc.motss, et al.] \"Princeton axes match...  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data_file2 = pd.read_csv('preprocessed_data_file2.csv')\n",
    "preprocessed_data_file2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now Split the data into Train and test. use 25% for test also do a stratify split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = preprocessed_data_file2['class_label'].values\n",
    "x = preprocessed_data_file2.drop(['class_label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test= train_test_split(x,y,random_state=42,stratify=y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_name</th>\n",
       "      <th>doc_contents</th>\n",
       "      <th>preprocessed_subject</th>\n",
       "      <th>preprocessed_email</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>preprocessed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16428</th>\n",
       "      <td>talk.politics.guns_55233.txt</td>\n",
       "      <td>['From: wwarf@silver.ucs.indiana.edu (Wayne J....</td>\n",
       "      <td>Re: Your Evil Tax Dollars at Work, was RE: AT...</td>\n",
       "      <td>silver ucs indiana edu psuvm psu edu psuvm psu...</td>\n",
       "      <td>your evil tax dollars work  was atf burns ranc...</td>\n",
       "      <td>Re: Your Evil Tax Dollars at Work, was RE: AT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10459</th>\n",
       "      <td>rec.sport.hockey_54284.txt</td>\n",
       "      <td>['From: drozinst@db.erau.edu (Drozinski Tim)\\n...</td>\n",
       "      <td>Re: Ulf and all...</td>\n",
       "      <td>erau edu philabs philips alpha erau edu erau e...</td>\n",
       "      <td>ulf and all    in article an ulf fan  and what...</td>\n",
       "      <td>Re: Ulf and all... erau edu philabs philips a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      document_name  \\\n",
       "16428  talk.politics.guns_55233.txt   \n",
       "10459    rec.sport.hockey_54284.txt   \n",
       "\n",
       "                                            doc_contents  \\\n",
       "16428  ['From: wwarf@silver.ucs.indiana.edu (Wayne J....   \n",
       "10459  ['From: drozinst@db.erau.edu (Drozinski Tim)\\n...   \n",
       "\n",
       "                                    preprocessed_subject  \\\n",
       "16428   Re: Your Evil Tax Dollars at Work, was RE: AT...   \n",
       "10459                                 Re: Ulf and all...   \n",
       "\n",
       "                                      preprocessed_email  \\\n",
       "16428  silver ucs indiana edu psuvm psu edu psuvm psu...   \n",
       "10459  erau edu philabs philips alpha erau edu erau e...   \n",
       "\n",
       "                                       preprocessed_text  \\\n",
       "16428  your evil tax dollars work  was atf burns ranc...   \n",
       "10459  ulf and all    in article an ulf fan  and what...   \n",
       "\n",
       "                                       preprocessed_data  \n",
       "16428   Re: Your Evil Tax Dollars at Work, was RE: AT...  \n",
       "10459   Re: Ulf and all... erau edu philabs philips a...  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Analyze your text data and pad the sequnce if required. \n",
    "Sequnce length is not restricted, you can use anything of your choice. \n",
    "you need to give the reasoning.\n",
    "\n",
    "4. Do Tokenizer i.e convert text into numbers. please be careful while doing it. \n",
    "if you are using tf.keras \"Tokenizer\" API, it removes the <b>\"_\"</b>, but we need that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train shape :-  (14121, 20)\n",
      "Y Test shape :-  (4707, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "Y_train_label = tf.keras.utils.to_categorical(LabelEncoder().fit_transform(y_train), 20)\n",
    "print('Y Train shape :- ',Y_train_label.shape)\n",
    "\n",
    "Y_test_label = tf.keras.utils.to_categorical(LabelEncoder().fit_transform(y_test), 20)\n",
    "print('Y Test shape :- ',Y_test_label.shape)                                                                                                                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "token = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^`{|}~\\t\\n')   # we are not mentioning '_' here.\n",
    "token.fit_on_texts(X_train['preprocessed_data'])\n",
    "\n",
    "vocab_size = len(token.word_index) + 1\n",
    "\n",
    "encoded_docs_train= token.texts_to_sequences(X_train['preprocessed_data'])\n",
    "encoded_docs_test= token.texts_to_sequences(X_test['preprocessed_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145865"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_for_padding=[] \n",
    "for i in encoded_docs_train:  \n",
    "    max_length_for_padding.append(len(i))  \n",
    "    \n",
    "max_len = max(max_length_for_padding)\n",
    "\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=max_len, padding='post')\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For every model use predefined Glove vectors. \n",
    "Don't train any word vectors while Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "file = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in file:\n",
    "    word = line.split()[0]\n",
    "    coef = np.asarray(line.split()[1:], dtype='float32')\n",
    "    embeddings_index[word] = coef\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "for word, i in token.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use \"categorical_crossentropy\" as Loss. \n",
    "\n",
    "8. Use Accuracy and Micro Avgeraged F1 score as your as Key metrics to evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "log_dir = \"documents/logs\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensor = TensorBoard(log_dir='documents/logs',histogram_freq=1,write_graph=True)\n",
    "\n",
    "class F1_score(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,validation_data):\n",
    "        self.validation_data = validation_data \n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f1 = []\n",
    "        self.aucs = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):    \n",
    "        predicted = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        target    = self.validation_data[1]\n",
    "        f1_values     = f1_score(target, predicted, average='micro')\n",
    "        auc_values    = roc_auc_score(target, predicted)\n",
    "        \n",
    "        self.f1.append(f1_values)\n",
    "        self.aucs.append(auc_values)\n",
    "        \n",
    "        print(' AUC = {0} and F1 Score = {1}'.format(auc_values,f1_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath = 'best_model_weights/best_model_L.h5', monitor ='accuracy' ,mode='auto', save_best_only = True , verbose=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "5. code the model's ( Model-1, Model-2 ) as discussed below \n",
    "and try to optimize that models.  \n",
    "\n",
    "6. For every model use predefined Glove vectors. \n",
    "<b>Don't train any word vectors while Training the model.</b>\n",
    "\n",
    "7. Use \"categorical_crossentropy\" as Loss. \n",
    "\n",
    "8. Use <b>Accuracy and Micro Avgeraged F1 score</b> as your as Key metrics to evaluate your model. \n",
    "\n",
    "9.  Use Tensorboard to plot the loss and Metrics based on the epoches.\n",
    "\n",
    "10. Please save your best model weights in to <b>'best_model_L.h5' ( L = 1 or 2 )</b>. \n",
    "\n",
    "11. You are free to choose any Activation function, learning rate, optimizer.\n",
    "But have to use the same architecture which we are giving below.\n",
    "\n",
    "12. You can add some layer to our architecture but you <b>deletion</b> of layer is not acceptable.\n",
    "\n",
    "13. Try to use <b>Early Stopping</b> technique or any of the callback techniques that you did in the previous assignments.\n",
    "\n",
    "14. For Every model save your model to image ( Plot the model) with shapes \n",
    "and inlcude those images in the notebook markdown cell, \n",
    "upload those imgages to Classroom. You can use \"plot_model\" \n",
    "please refer <a href='https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model'>this</a> if you don't know how to plot the model with shapes. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-1: Using 1D convolutions with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 8647)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 8647, 100)    14586500    input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, 8631, 13)     22113       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, 8630, 13)     23413       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, 8632, 13)     20813       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 25893, 13)    0           conv1d_78[0][0]                  \n",
      "                                                                 conv1d_79[0][0]                  \n",
      "                                                                 conv1d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 8631, 13)     0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 8613, 12)     2976        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 8618, 12)     2196        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 8617, 12)     2352        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 25848, 12)    0           conv1d_81[0][0]                  \n",
      "                                                                 conv1d_82[0][0]                  \n",
      "                                                                 conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 8616, 12)     0           concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8616, 12)     0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 2872, 12)     0           dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 2861, 15)     2175        max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 2861, 15)     2175        max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 5722, 15)     0           conv1d_84[0][0]                  \n",
      "                                                                 conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 85830)        0           concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 85830)        0           flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 124)          10643044    dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 20)           2500        dense_42[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,310,257\n",
      "Trainable params: 25,310,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input_text: InputLayers\n",
    "inputLayers =  Input(shape=(max_len,),dtype='int32')\n",
    "\n",
    "# embedding: Embedding\n",
    "emd = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=True)(inputLayers)\n",
    "\n",
    "# three layers of Conv1D with filter size \n",
    "x1 = Conv1D(13,17,  kernel_initializer=tf.keras.initializers.HeUniform(), activation='relu')(emd)\n",
    "x2 = Conv1D(13,18,  kernel_initializer=tf.keras.initializers.HeUniform(), activation='relu')(emd)\n",
    "x3 = Conv1D(13,16,  kernel_initializer=tf.keras.initializers.HeUniform(), activation='relu')(emd)\n",
    "\n",
    "# concatenate1: Concatenate_above_three_layers: concatenate\n",
    "concatenated_layer1 = concatenate([x1,x2,x3],axis=1)\n",
    "\n",
    "# max_poolLayer1 : MaxPooling1D\n",
    "max_pool1 = MaxPooling1D(3)(concatenated_layer1)\n",
    "\n",
    "# Conv1D with filter size 9,4,5\n",
    "x4=Conv1D(12,19,kernel_initializer=tf.keras.initializers.HeUniform(),activation='relu')(max_pool1)\n",
    "x5=Conv1D(12,14,kernel_initializer=tf.keras.initializers.HeUniform(),activation='relu')(max_pool1)\n",
    "x6=Conv1D(12,15,kernel_initializer=tf.keras.initializers.HeUniform(),activation='relu')(max_pool1)\n",
    "\n",
    "# concatenate2: Concatenate_above_three_layers: concatenate\n",
    "concatenated_layer2 =concatenate([x4,x5,x6],axis=1)\n",
    "\n",
    "# max_poolLayer2 : MaxPooling1D\n",
    "max_pool2 = MaxPooling1D(3)(concatenated_layer2)\n",
    "\n",
    "# Dropout: dropout\n",
    "drop_out1=Dropout(0.25)(max_pool2)\n",
    "\n",
    "max_pool3=MaxPooling1D(3)(drop_out1)\n",
    "\n",
    "x7=Conv1D(15,12,kernel_initializer=tf.keras.initializers.HeUniform() ,activation='relu')(max_pool3)\n",
    "x8=Conv1D(15,12,kernel_initializer=tf.keras.initializers.HeUniform() ,activation='relu')(max_pool3)\n",
    "\n",
    "concatenated_layer3 =concatenate([x7,x8],axis=1)\n",
    "flatten=Flatten()(concatenated_layer3)\n",
    "\n",
    "# Dropout: dropout\n",
    "drop_out2=Dropout(0.25)(flatten)\n",
    "\n",
    "# Dense1 : Dense\n",
    "dense_layer1=Dense(124, activation='relu')(drop_out2)\n",
    "outputLayer=Dense(20, activation='softmax')(dense_layer1)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "early_stopping = EarlyStopping(monitor='accuracy', min_delta=0.001, patience=2, verbose=1)\n",
    "score=F1_score(validation_data=(padded_docs_test,Y_test_label))\n",
    "\n",
    "# OutputLayer : Dense\n",
    "model = Model(inputs=inputLayers,outputs=outputLayer)\n",
    "# compile the model\n",
    "\n",
    "history=model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 2.2641 - accuracy: 0.2523 AUC = 0.5874998871616883 and F1 Score = 0.29850746268656714\n",
      "\n",
      "Epoch 00001: accuracy did not improve from 0.42872\n",
      "331/331 [==============================] - 1166s 4s/step - loss: 2.2641 - accuracy: 0.2523 - val_loss: 1.6400 - val_accuracy: 0.4449\n",
      "Epoch 2/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 1.0788 - accuracy: 0.6245 AUC = 0.763603551647611 and F1 Score = 0.6625194401244168\n",
      "\n",
      "Epoch 00002: accuracy improved from 0.42872 to 0.62446, saving model to best_model_weights\\best_model_L.h5\n",
      "331/331 [==============================] - 1158s 3s/step - loss: 1.0788 - accuracy: 0.6245 - val_loss: 0.9134 - val_accuracy: 0.7114\n",
      "Epoch 3/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.8229 AUC = 0.8155489902777248 and F1 Score = 0.7310311865216872\n",
      "\n",
      "Epoch 00003: accuracy improved from 0.62446 to 0.82295, saving model to best_model_weights\\best_model_L.h5\n",
      "331/331 [==============================] - 1082s 3s/step - loss: 0.5166 - accuracy: 0.8229 - val_loss: 0.8133 - val_accuracy: 0.7400\n",
      "Epoch 4/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9212 AUC = 0.8525238750322558 and F1 Score = 0.7631046119235095\n",
      "\n",
      "Epoch 00004: accuracy improved from 0.82295 to 0.92125, saving model to best_model_weights\\best_model_L.h5\n",
      "331/331 [==============================] - 1083s 3s/step - loss: 0.2489 - accuracy: 0.9212 - val_loss: 0.8453 - val_accuracy: 0.7613\n",
      "Epoch 5/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9564 AUC = 0.8505287742244908 and F1 Score = 0.7544883303411132\n",
      "\n",
      "Epoch 00005: accuracy improved from 0.92125 to 0.95637, saving model to best_model_weights\\best_model_L.h5\n",
      "331/331 [==============================] - 1080s 3s/step - loss: 0.1427 - accuracy: 0.9564 - val_loss: 0.8826 - val_accuracy: 0.7533\n",
      "Epoch 6/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9828 AUC = 0.875102161762529 and F1 Score = 0.7856986660835339\n",
      "\n",
      "Epoch 00006: accuracy improved from 0.95637 to 0.98281, saving model to best_model_weights\\best_model_L.h5\n",
      "331/331 [==============================] - 1081s 3s/step - loss: 0.0635 - accuracy: 0.9828 - val_loss: 0.9606 - val_accuracy: 0.7899\n",
      "Epoch 7/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9864 AUC = 0.8648674446310354 and F1 Score = 0.7602046369870469\n",
      "\n",
      "Epoch 00007: accuracy improved from 0.98281 to 0.98640, saving model to best_model_weights\\best_model_L.h5\n",
      "331/331 [==============================] - 1221s 4s/step - loss: 0.0467 - accuracy: 0.9864 - val_loss: 1.1527 - val_accuracy: 0.7581\n",
      "Epoch 8/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9850 AUC = 0.8748659981158571 and F1 Score = 0.7811652381469024\n",
      "\n",
      "Epoch 00008: accuracy did not improve from 0.98640\n",
      "331/331 [==============================] - 1082s 3s/step - loss: 0.0526 - accuracy: 0.9850 - val_loss: 1.1343 - val_accuracy: 0.7859\n",
      "Epoch 9/10\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9856 AUC = 0.8608088269026778 and F1 Score = 0.7470035633300939\n",
      "\n",
      "Epoch 00009: accuracy did not improve from 0.98640\n",
      "331/331 [==============================] - 1078s 3s/step - loss: 0.0484 - accuracy: 0.9856 - val_loss: 1.3514 - val_accuracy: 0.7505\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2722ac83520>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs_train, Y_train_label,validation_split=0.25, epochs=10, verbose=1,callbacks=[tensor,early_stopping,score,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 31s 206ms/step - loss: 1.3573 - accuracy: 0.7434\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs_test, Y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.33609366416931"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cg4L1V4Yv1d"
   },
   "source": [
    "### Model-2 : Using 1D convolutions with character embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Djg4YVA3oQx"
   },
   "source": [
    "<pre>\n",
    "<pre><img src=\"https://i.ytimg.com/vi/CNY8VjJt-iQ/maxresdefault.jpg\" width=\"70%\">\n",
    "Here are the some papers based on Char-CNN\n",
    " 1. Xiang Zhang, Junbo Zhao, Yann LeCun. <a href=\"http://arxiv.org/abs/1509.01626\">Character-level Convolutional Networks for Text Classification</a>.NIPS 2015\n",
    " 2. Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush. <a href=\"https://arxiv.org/abs/1508.06615\">Character-Aware Neural Language Models</a>. AAAI 2016\n",
    " 3. Shaojie Bai, J. Zico Kolter, Vladlen Koltun. <a href=\"https://arxiv.org/pdf/1803.01271.pdf\">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</a>\n",
    " 4. Use the pratrained char embeddings <a href='https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt'>https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "token = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^`{|}~\\t\\n',char_level=True,lower=True,num_words=10000)   # we are not mentioning '_' here.\n",
    "token.fit_on_texts(X_train['preprocessed_data'])\n",
    "\n",
    "vocab_size = len(token.word_index) + 1\n",
    "\n",
    "encoded_docs_train= token.texts_to_sequences(X_train['preprocessed_data'])\n",
    "encoded_docs_test= token.texts_to_sequences(X_test['preprocessed_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_for_padding=[] \n",
    "for i in encoded_docs_train:  \n",
    "    max_length_for_padding.append(len(i))  \n",
    "    \n",
    "max_len = max(max_length_for_padding)\n",
    "\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=max_len, padding='post')\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 94 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "file = open('glove.840B.300d-char.txt')\n",
    "for line in file:\n",
    "    word = line.split()[0]\n",
    "    coef = np.asarray(line.split()[1:], dtype='float32')\n",
    "    embeddings_index[word] = coef\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "\n",
    "for word, i in token.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 56440)]           0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 56440, 300)        21300     \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 56425, 36)         172836    \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 56414, 24)         10392     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 28207, 24)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 28192, 32)         12320     \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 28181, 16)         6160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 14090, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 14083, 28)         3612      \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 14080, 18)         2034      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 7040, 18)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 126720)            0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 8)                 1013768   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 1,315,434\n",
      "Trainable params: 1,315,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputLayers =  Input(shape=(max_len,),dtype='int32')\n",
    "\n",
    "# embedding: Embedding layer to get Char \n",
    "emd = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=True)(inputLayers)\n",
    "\n",
    "# Conv1D with filter size 36,24\n",
    "f1 = Conv1D(filters = 36, kernel_size= 16, activation='relu')(emd)\n",
    "f2 = Conv1D(filters = 24, kernel_size= 12, activation='relu')(f1)\n",
    "\n",
    "mp1 = MaxPooling1D(pool_size=2)(f2)\n",
    "\n",
    "# Conv1D with filter size 32,16\n",
    "f3 = Conv1D(filters = 32, kernel_size= 16, activation='relu')(mp1)\n",
    "f4 = Conv1D(filters = 16, kernel_size= 12, activation='relu')(f3)   \n",
    "mp2 = MaxPooling1D(pool_size=2)(f4)\n",
    "\n",
    "# Conv1D with filter size 28,18\n",
    "f5 = Conv1D(filters = 28, kernel_size= 8, activation='relu')(mp2)\n",
    "f6 = Conv1D(filters = 18, kernel_size= 4, activation='relu')(f5)\n",
    "\n",
    "mp3 = MaxPooling1D(pool_size=2)(f6)\n",
    "flat1 = Flatten()(mp3)\n",
    "\n",
    "# Dense1 : Dense\n",
    "d1 = Dense(8,activation ='relu')(flat1)\n",
    "d2 = Dense(32,activation ='relu')(d1)\n",
    "\n",
    "# Dropout: dropout\n",
    "drop1 = Dropout(0.20)(d2)\n",
    "\n",
    "# Dense2 : Dense\n",
    "d3 = Dense(128,activation ='relu')(drop1)\n",
    "d4 = Dense(256,activation ='relu')(d3)\n",
    "\n",
    "# Dropout2: dropout\n",
    "drop2 = Dropout(0.20)(d4)\n",
    "\n",
    "# Dense3 : Dense\n",
    "d5 = Dense(128,activation ='relu')(drop2)\n",
    "\n",
    "# OutputLayer\n",
    "outputLayer = Dense(20, activation ='softmax')(d5)\n",
    "\n",
    "model = Model(inputs = inputLayers,outputs = outputLayer)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "earlystopping = EarlyStopping(monitor='accuracy', min_delta=0.005, patience=2, verbose=1)\n",
    "score=F1_score(validation_data=(padded_docs_test,Y_test_label))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "442/442 [==============================] - ETA: 0s - loss: 2.9654 - accuracy: 0.0696  AUC = 0.5 and F1 Score = 0.0\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.06961, saving model to best_model_weights\\best_model_L.h5\n",
      "442/442 [==============================] - 8594s 19s/step - loss: 2.9654 - accuracy: 0.0696\n",
      "Epoch 2/10\n",
      "442/442 [==============================] - ETA: 0s - loss: 2.9312 - accuracy: 0.0814  AUC = 0.5 and F1 Score = 0.0\n",
      "\n",
      "Epoch 00002: accuracy improved from 0.06961 to 0.08144, saving model to best_model_weights\\best_model_L.h5\n",
      "442/442 [==============================] - 8546s 19s/step - loss: 2.9312 - accuracy: 0.0814\n",
      "Epoch 3/10\n",
      "442/442 [==============================] - ETA: 0s - loss: 2.9210 - accuracy: 0.0836  AUC = 0.5 and F1 Score = 0.0\n",
      "\n",
      "Epoch 00003: accuracy improved from 0.08144 to 0.08356, saving model to best_model_weights\\best_model_L.h5\n",
      "442/442 [==============================] - 8548s 19s/step - loss: 2.9210 - accuracy: 0.0836\n",
      "Epoch 4/10\n",
      "442/442 [==============================] - ETA: 0s - loss: 2.9130 - accuracy: 0.0840  AUC = 0.5 and F1 Score = 0.0\n",
      "\n",
      "Epoch 00004: accuracy improved from 0.08356 to 0.08399, saving model to best_model_weights\\best_model_L.h5\n",
      "442/442 [==============================] - 8559s 19s/step - loss: 2.9130 - accuracy: 0.0840\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27200d5ad90>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs_train, Y_train_label, epochs=10, verbose=1,callbacks=[tensor,earlystopping,score,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 353s 2s/step - loss: 2.9106 - accuracy: 0.0884\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs_test, Y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.837901055812836"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Difference between Character Embedding and Word Embedding is that Character Embedding can build any word as long as those character are included.\n",
    "2. There are about 71 English-language characters in common usage if we include all punctuation marks. By contrast, a vocabulary is many thousands of words. Beacuse of this, Word Embedding(model 1) works better than character Embedding(Model 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
